{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d74df9a-7d95-4b45-8f53-7c22b8694a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Any, Dict, Callable, Iterable\n",
    "import pandas as pd\n",
    "import json\n",
    "import datasets\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638f8eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: qa_srl/plain_text\n",
      "Reusing dataset qa_srl (/home/nlp/kleinay/.cache/huggingface/datasets/kleinay___qa_srl/plain_text/1.0.0/9aaf099b628da9c576ebbc49bd242c93d0e6cc79ffdb2e0e1d3daf409f696820)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550763d35d2d41d192a8e36245e2547d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# qanom = datasets.load_dataset(\"biu-nlp/qanom\")\n",
    "# qasrl = datasets.load_dataset(\"biu-nlp/qa_srl2018\", \"v2\")\n",
    "qasrl = datasets.load_dataset(\"kleinay/qa_srl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ed5626",
   "metadata": {},
   "source": [
    "### Prepare qasrl_slots for DFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "07694be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.concatenate_datasets([\n",
    "    # qanom['train'], qanom['validation'] \n",
    "    # qasrl['train'], qasrl['validation'] \n",
    "    qanom['train'], qanom['validation'], qasrl['train'], qasrl['validation'] \n",
    "]) \n",
    "\n",
    "questions = [q for q in data['question'] if q]\n",
    "slots = list(zip(*questions))\n",
    "slots = [sorted(set(slot)) for slot in slots]\n",
    "slots_dict = {\"wh\": slots[0],\n",
    "              \"aux\": slots[1],\n",
    "              \"subj\": slots[2],\n",
    "              \"verb\": slots[3],\n",
    "              \"obj\": slots[4],\n",
    "              \"prep\": slots[5],\n",
    "              \"obj2\": slots[6],\n",
    "              \"?\": slots[7]}\n",
    "\n",
    "# slots_dict.pop(\"verb\")\n",
    "import json\n",
    "# json.dump(slots_dict, open(\"seq2seq_constrained_decoding/qasrl_slots.json\", \"w\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4277e39",
   "metadata": {},
   "source": [
    "Verify that there no conflicts between adjecent slots, making the qasrl_question_dfa non-deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b914966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conflicts for wh vs. aux: []\n",
      "Conflicts for aux vs. subj: [('_', '_')]\n",
      "Conflicts for subj vs. verb: []\n",
      "Conflicts for verb vs. obj: []\n",
      "Conflicts for obj vs. prep: [('_', '_')]\n",
      "Conflicts for prep vs. obj2: [('_', '_'), ('as doing', 'doing'), ('by doing', 'doing'), ('in doing', 'doing'), ('of doing', 'doing'), ('on doing', 'doing'), ('to do', 'do'), ('to doing', 'doing'), ('with doing', 'doing')]\n",
      "Conflicts for obj2 vs. ?: []\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "def is_conflicting(slot1, slot2):\n",
    "    tok1 = slot1.split(\" \")\n",
    "    tok2 = slot2.split(\" \")\n",
    "    return tok1[-1] == tok2[0]\n",
    "\n",
    "def find_conflicts(preps, obj2):\n",
    "    return [(s1, s2) for s1, s2 in itertools.product(preps, obj2)\n",
    "            if is_conflicting(s1, s2)] \n",
    "    \n",
    "slot_names = list(slots_dict.keys())\n",
    "for sl1, sl2 in zip(slot_names[:-1], slot_names[1:]):\n",
    "    print(f\"Conflicts for {sl1} vs. {sl2}: {find_conflicts(slots_dict[sl1], slots_dict[sl2])}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eec59bd",
   "metadata": {},
   "source": [
    "Reomve conflicts between prep and obj2 by striping obj2 words from preps, then save and override qasrl_slots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5cbe8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve conflicts in prep--obj2 slots \n",
    "\n",
    "for i, prep in enumerate(slots_dict[\"prep\"]):\n",
    "    for obj in slots_dict[\"obj2\"]:\n",
    "        if prep.endswith(f\" {obj}\"):\n",
    "            new_prep = prep[:-(len(obj)+1)]\n",
    "            slots_dict[\"prep\"][i] = new_prep\n",
    "            break\n",
    "slots_dict[\"prep\"] = list(sorted(set(slots_dict[\"prep\"])))\n",
    "json.dump(slots_dict, open(\"seq2seq_constrained_decoding/qasrl_slots.json\", \"w\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b9e1bc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'' in slots_dict[\"prep\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5f643d",
   "metadata": {},
   "source": [
    "Inspect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2672eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_with_sep = [q for q in questions if q and \"~!~\" in q[3]]\n",
    "q_with_sep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2b2c806a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wh': 0, 'aux': 0, 'subj': 0, 'verb': 0, 'obj': 0, 'prep': 0, 'obj2': 0, '?': 0}\n",
      "set()\n",
      "{'have been', 'have', 'not have been', 'not', 'be', 'not have', 'been', 'not be', 'being'}\n"
     ]
    }
   ],
   "source": [
    "def countin(l, p):\n",
    "    return len([s for s in l if p in s])\n",
    "\n",
    "print({sl:countin(l, \"~!~\") for sl,l in slots_dict.items()})\n",
    "\n",
    "verb_prefixes= set([' '.join(t.split(\" \")[:-1]) for t in v_vs if len(t.split(\" \"))>1])\n",
    "print(verb_prefixes)\n",
    "verb_prefixes= set([' '.join(t.split(\" \")[:-1]) for t in n_vs if len(t.split(\" \"))>1])\n",
    "print(verb_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8ab075d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 215432})\n",
      "Counter({1: 13592, 2: 2239, 3: 63, 4: 1})\n",
      "{('not', 'have'), ('been',), ('be',), ('not', 'have', 'been'), ('have',), ('being',), ('have', 'been'), ('not', 'be'), ('not',)}\n"
     ]
    }
   ],
   "source": [
    "v_qs = qasrl['train'][\"question\"]\n",
    "n_qs = qanom['train'][\"question\"]\n",
    "\n",
    "v_vs = [q[3] for q in v_qs]\n",
    "n_vs = [q[3] for q in n_qs if q]\n",
    "v_preps = [q[5] for q in v_qs]\n",
    "n_preps = [q[5] for q in n_qs if q]\n",
    "print(Counter(map(lambda v:len(v.split(\" \")), v_vs)))\n",
    "print(Counter(map(lambda v:len(v.split(\" \")), n_vs)))\n",
    "\n",
    "#verb prefixes\n",
    "# print(set([t.split(\" \")[0] for t in v_vs if len(t.split(\" \"))>1]))\n",
    "print(set([tuple(t.split(\" \")[:-1]) for t in n_vs if len(t.split(\" \"))>1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "67ceba7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"can't\", 'has', \"didn't\", \"wasn't\", 'did', 'was', 'can', \"hadn't\", 'would', 'will', \"won't\", \"hasn't\", 'does', 'had', 'is', \"shouldn't\", \"isn't\", 'should', \"wouldn't\", 'might', '_', \"doesn't\"}\n",
      "{\"can't\", 'has', \"didn't\", \"wasn't\", 'did', 'was', 'can', 'would', 'will', \"won't\", \"hasn't\", 'does', 'had', 'is', \"shouldn't\", \"isn't\", 'should', \"wouldn't\", 'might', '_', \"doesn't\"}\n",
      "{'something', 'somewhere', 'doing', '_', 'do', 'someone'}\n",
      "{'something', 'somewhere', 'doing', '_', 'do', 'someone'}\n"
     ]
    }
   ],
   "source": [
    "v_auxs = [q[1] for q in v_qs]\n",
    "n_auxs = [q[1] for q in n_qs if q]\n",
    "\n",
    "print(set(v_auxs))\n",
    "print(set(n_auxs))\n",
    "\n",
    "v_obj2s = [q[6] for q in v_qs]\n",
    "n_obj2s = [q[6] for q in n_qs if q]\n",
    "\n",
    "print(set(v_obj2s))\n",
    "print(set(n_obj2s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af5b901",
   "metadata": {},
   "source": [
    "## Contextualizing QASRL datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e7af674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:44:15,304 - DEBUG - urllib3.connectionpool - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2022-05-01 16:44:16,062 - DEBUG - urllib3.connectionpool - https://huggingface.co:443 \"HEAD /biu-nlp/contextualizer_qasrl/resolve/main/vocab.json HTTP/1.1\" 200 0\n",
      "2022-05-01 16:44:16,072 - DEBUG - urllib3.connectionpool - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2022-05-01 16:44:16,736 - DEBUG - urllib3.connectionpool - https://huggingface.co:443 \"HEAD /biu-nlp/contextualizer_qasrl/resolve/main/merges.txt HTTP/1.1\" 200 0\n",
      "2022-05-01 16:44:16,745 - DEBUG - urllib3.connectionpool - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2022-05-01 16:44:17,428 - DEBUG - urllib3.connectionpool - https://huggingface.co:443 \"HEAD /biu-nlp/contextualizer_qasrl/resolve/main/added_tokens.json HTTP/1.1\" 404 0\n",
      "2022-05-01 16:44:17,437 - DEBUG - urllib3.connectionpool - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2022-05-01 16:44:18,122 - DEBUG - urllib3.connectionpool - https://huggingface.co:443 \"HEAD /biu-nlp/contextualizer_qasrl/resolve/main/special_tokens_map.json HTTP/1.1\" 200 0\n",
      "2022-05-01 16:44:18,130 - DEBUG - urllib3.connectionpool - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2022-05-01 16:44:18,801 - DEBUG - urllib3.connectionpool - https://huggingface.co:443 \"HEAD /biu-nlp/contextualizer_qasrl/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2022-05-01 16:44:18,811 - DEBUG - urllib3.connectionpool - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2022-05-01 16:44:19,513 - DEBUG - urllib3.connectionpool - https://huggingface.co:443 \"HEAD /biu-nlp/contextualizer_qasrl/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2022-05-01 16:44:19,639 - DEBUG - urllib3.connectionpool - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2022-05-01 16:44:20,322 - DEBUG - urllib3.connectionpool - https://huggingface.co:443 \"HEAD /biu-nlp/contextualizer_qasrl/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2022-05-01 16:44:20,334 - DEBUG - urllib3.connectionpool - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2022-05-01 16:44:21,003 - DEBUG - urllib3.connectionpool - https://huggingface.co:443 \"HEAD /biu-nlp/contextualizer_qasrl/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n"
     ]
    }
   ],
   "source": [
    "# Code copied from the implementation in `run_parsing_model`\n",
    "\n",
    "from roleqgen.question_translation import QuestionTranslator\n",
    "contextualizer = QuestionTranslator.from_pretrained(\"biu-nlp/contextualizer_qasrl\", device_id=0)\n",
    "\n",
    "# Prepare contexts (inputs) for contextualizer\n",
    "def as_input_for_contextualizer(qa):\n",
    "    question = ' '.join(qa['question']).replace(' _', '').replace(' ?', '?')\n",
    "    return {'proto_question': question, \n",
    "            'predicate_lemma': qa['verb_form'],\n",
    "            'predicate_span': f\"{qa['predicate_idx']}:{qa['predicate_idx'] + 1}\",\n",
    "            'text': qa['sentence']}\n",
    "\n",
    "# Take contextualized slots from contextualized-question (co_q)  \n",
    "def to_filled_slots(orig_slots, co_q: str) -> List[str]:\n",
    "    # context can be at slots SUBJ (2), OBJ (4), OBJ2 (6); take from contextualized question\n",
    "    if not orig_slots:\n",
    "        return orig_slots\n",
    "    wh, aux, subj, verb, obj, prep, obj2, _ = orig_slots\n",
    "    co_q = without_suffix(co_q, '?') + ' '\n",
    "    if wh not in co_q or f\"{verb} \" not in co_q or (aux != \"_\" and aux not in co_q):\n",
    "        return orig_slots\n",
    "    \n",
    "    pre_v, post_v = co_q.split(f\"{verb} \", 1)\n",
    "    # subj is the part before verb after prefix\n",
    "    subj = without_prefix(pre_v, wh.title()).lstrip()\n",
    "    subj = without_prefix(subj, aux).strip()\n",
    "    # if prep is not copied within co_q, cannot identify objects\n",
    "    if prep != \"_\" and prep not in co_q:\n",
    "        return [wh, aux, subj, verb, obj, prep, obj2, '?']\n",
    "    # if at most one object is present, can easily know which is it\n",
    "    if obj != \"_\" and obj2 == \"_\":\n",
    "        obj = without_suffix(post_v.rstrip(), prep).strip()\n",
    "    elif obj == \"_\" and obj2 != \"_\":\n",
    "        obj2 = without_prefix(post_v.rstrip(), prep).strip()\n",
    "    # if both objects are present, prep (in between) should be non empty\n",
    "    elif obj != \"_\" and obj2 != \"_\":\n",
    "        obj, obj2 = post_v.split(f\" {prep} \", 1)\n",
    "    return [wh, aux, subj, verb, obj, prep, obj2, '?']      \n",
    "    \n",
    "def contextualize(orig_dataset):\n",
    "    # orig_dataset = raw_datasets[split]\n",
    "    # Prepare contextualizer inputs from datatset\n",
    "    inputs_for_contextualizer = orig_dataset.map(as_input_for_contextualizer, remove_columns=[\n",
    "        'sentence', 'sent_id', 'predicate_idx', 'predicate', 'is_verbal', 'verb_form', 'question', 'answers', 'answer_ranges'])\n",
    "    inputs_for_contextualizer = inputs_for_contextualizer.to_pandas().to_dict('records')\n",
    "    # Run contextualizer\n",
    "    contextualized_questions = contextualizer.predict(inputs_for_contextualizer)\n",
    "    # Modify questions in dataset\n",
    "    def contextualize_dataset(example, idx):\n",
    "        example['question'] = to_filled_slots(example['question'], contextualized_questions[idx])\n",
    "        return example  \n",
    "    ret = orig_dataset.map(\n",
    "        contextualize_dataset,\n",
    "        with_indices=True,\n",
    "        batched=True,\n",
    "        load_from_cache_file=False,\n",
    "        desc=f\"contextualizing questions of the dataset\"\n",
    "    )\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c4abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on QASRL datasets\n",
    "\n",
    "qanom_dataset = datasets.load_dataset(\"biu-nlp/qanom\")\n",
    "for split in qanom_dataset:\n",
    "    orig_dataset = qanom_dataset[split]\n",
    "    new_dataset = contextualize(orig_dataset)\n",
    "    new_dataset.to_csv(f\"qanom_{split}_contextualized.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4919523a",
   "metadata": {},
   "source": [
    "# Evaluate nrl-parser\n",
    "\n",
    "### 1. Prepare input for nrl-parser (jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b06ac173",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = set(qasrl['test']['sentence'])\n",
    "dicts = [{\"sentence\": s} for s in test_sentences]\n",
    "with open(\"qasrl_gs.test.jsonl\", \"w\") as fout:\n",
    "    for dic in dicts:\n",
    "        fout.write(json.dumps(dic) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0199cb5b",
   "metadata": {},
   "source": [
    "### 2. translate parser output (jsonl) into csv \n",
    "This includes decoding the question to 7-slots.\n",
    "We will try to use our DFA code for that.\n",
    "\n",
    "*Conclusion*: That's not possible, as the nrl parser leave no traces for empty slots (_) so the DFA cannot parse it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38baf88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wh': 'why',\n",
       " 'aux': 'was',\n",
       " 'subj': 'someone',\n",
       " 'verb': 'cited',\n",
       " 'obj': '_',\n",
       " 'prep': '_',\n",
       " 'obj2': '_'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pipeline import get_markers_for_model\n",
    "from dfa_fill_qasrl_slots import dfa_fill_qasrl_slots, extract_is_negated, SLOT_TO_STATE\n",
    "\n",
    "special_tokens = get_markers_for_model(True)\n",
    "from constrained_decoding.qasrl import get_qasrl_question_dfa\n",
    "question_dfa = get_qasrl_question_dfa(constrain_verb=False)\n",
    "from strings_to_objects_parser import StringsToObjectsParser\n",
    "str2objParser = StringsToObjectsParser(special_tokens, None)\n",
    "# str2objParser._get_question_slots(\"Why was someone cited?\")\n",
    "dfa_fill_qasrl_slots(\"why was someone cited _ _ _ ?\", question_dfa)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "efc159d1a32205ec9db73f14cd17171e4e15bf26729f2bc17d41c8a634b0d97c"
  },
  "kernelspec": {
   "display_name": "seq2seq-qasrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
