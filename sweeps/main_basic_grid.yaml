name: basic grid for main experiment - role order, joint
method: grid
metric:
  name: Labled Arg f1
  goal: maximize
# program: run_summarization.py # for baselines
program: joint_train_experiment.py # for joint

parameters:
  num_train_epochs:
    value: 20
  preprocess_output_func:
    values:
      - all_by_role_ordering
      # - all_by_answer_ordering
      # - permutate_all
      # - permutate_sample_num_of_qas
      # - permutate_sample_fixed
      # - all_shuffled
      # - all_random_order
  learning_rate:
    values: 
      - 0.001
      - 0.005
      # - 0.01
  dropout_rate:
    values: 
      - 0.1
      - 0.15
  gradient_accumulation_steps:
    values: 
      - 8
      - 14
  per_device_train_batch_size:
    value: 12
  seed:
    value: 44
  num_beams:
    value: 5
  source_prefix: 
    value: 'parse: '

command:
  - ${env}
  - ${interpreter}
  - ${program}
  - ${args}
  # add params
  - '--do_train'
  - '--do_eval'
  - '--report_to' 
  - 'wandb'
  - '--overwrite_output_dir'
  - '--predict_with_generate'
  - '--append_verb_form'
  - '--use_bilateral_predicate_marker'
  - '--fp16'
  - '--load_best_model_at_end'
  - '--do_eval_on' 
  - 'validation'
  - '--save_strategy' 
  - 'steps'
  - '--logging_strategy' 
  - 'steps'
  - '--evaluation_strategy' 
  - 'steps'
  - '--logging_steps' 
  - 500
  - '--eval_steps' 
  - 500
  - '--save_steps' 
  - 500
  - '--metric_for_best_model' 
  - 'eval_loss'
  - '--predicate_marker_type' 
  - 'generic'
  - '--model_type' 
  - 't5'
  - '--preprocess_input_func' 
  - 'input_predicate_marker'
  - '--output_dir' 
  - '/home/nlp/kleinay/tmp/t5-tst-summarization/joint/main_grid'  
  - '--model_name_or_path' 
  - 't5-small'
  # - '--dataset_name' 
  # - 'biu-nlp/qanom'
  # - 'kleinay/qa_srl'

  # - '--limit_train_data'
  # - 0.07

  - '--wandb_run_name' 
  - 'main grid sweep joint (eval on dev)  ' 
