{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datasets\n",
    "import qanom\n",
    "from qanom.annotations.common import read_annot_csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring QA-level Condfidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"kleinay/qanom-seq2seq-model-joint\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kleinay/qanom-seq2seq-model-joint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = \"parse: yesterday , you<extra_id_10> took<extra_id_10> my hand with your fork to ask me out .<extra_id_1> took\"\n",
    "input_ids = tokenizer(input_seq, return_tensors=\"pt\").input_ids\n",
    "decoder_input_ids = tokenizer(\"<pad>\", add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "# decoder_input_ids = tokenizer(\"<pad> when did someone take something _ _?<extra_id_7> yesterday</s>\",  \n",
    "#                               add_special_tokens=False, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> when did someone take something _ _?<extra_id_7> yesterday<extra_id_9> who _ _ took something _ _?<extra_id_7> you<extra_id_9> what did someone take _ _ _?<extra_id_7> my hand with your fork<extra_id_9> why did someone take something _ _?<extra_id_7> to ask me out</s><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "model.branching_strategy = \"standard_beam_search\"\n",
    "outputs = model.generate(input_ids, decoder_input_ids=decoder_input_ids, \n",
    "                         max_length=100,#len(decoder_input_ids[0]), \n",
    "                         output_scores=True,\n",
    "                         num_beams=3,\n",
    "                         num_return_sequences=3,\n",
    "                         return_dict_in_generate=True)\n",
    "# decoded_output = tokenizer.decode(outputs.sequences[2])\n",
    "# print(decoded_output)\n",
    "# decoded_output = tokenizer.decode(outputs.sequences[1])\n",
    "# print(decoded_output)\n",
    "decoded_output = tokenizer.decode(outputs.sequences[1])\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0379, -0.0514, -0.0531])\n",
      "tensor([0.9628, 0.9499, 0.9483])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.sequences_scores)\n",
    "print(outputs.sequences_scores.exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 74, 32101])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note: we only get `scores` for tokens that was generated during this decoidng, substracting tokens \n",
    "# that were given as input by `decoder_input_ids`. This can explain why \"<pad>\" is always not included.  \n",
    "scores = torch.stack(outputs.scores)\n",
    "scores = scores.transpose(0,1)\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([74])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.stack(outputs.beam_indices[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (74) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/tmp/ipykernel_38452/338929268.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# we should use this new functions to get transition probabilities:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m trs_bs = model.compute_transition_beam_scores(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbeam_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/seq2seq-qasrl/lib/python3.8/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mcompute_transition_beam_scores\u001b[0;34m(self, sequences, scores, beam_indices, eos_token_id)\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0mbeam_sequence_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0;31m# compute real indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeam_sequence_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m         \u001b[0;31m# gather scores and run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mtransition_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (74) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# we should use this new functions to get transition probabilities:\n",
    "trs_bs = model.compute_transition_beam_scores(\n",
    "    sequences=outputs.sequences,\n",
    "    scores=outputs.scores, \n",
    "    beam_indices=outputs.beam_indices\n",
    ")\n",
    "# Following https://github.com/huggingface/transformers/issues/15869\n",
    "print(\"Summ:\", torch.sum(trs_bs, dim=1), \"Expected:\", outputs.sequences_scores)\n",
    "print(\"Sum/length:\", torch.sum(trs_bs, dim=1)/len(outputs.beam_indices[0]), \"Expected:\", outputs.sequences_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(outputs.beam_indices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Manual Posterior Computation\n",
    "\n",
    "We have implemented a computation of \"sum of log probabilites\" in `QASRLSeq2SeqModel.get_sequence_score`.\n",
    "\n",
    "Let's test whether it is similar to the sequence_score returned from generate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/kleinay/tmp/ipykernel_38452/1711341721.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output_ids = torch.tensor(outputs.sequences)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9717)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "import sys, importlib\n",
    "# sys.path.append(\"..\")\n",
    "import seq2seq_model\n",
    "importlib.reload(seq2seq_model)\n",
    "from seq2seq_model import QASRLSeq2SeqModel\n",
    "\n",
    "model_name_or_path = \"kleinay/qanom-seq2seq-model-joint\"\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = QASRLSeq2SeqModel.from_pretrained(model_name_or_path,config=config)\n",
    "# generated = model.generate(input_ids, max_length=120, return_dict_in_generate=True)\n",
    "output_ids = torch.tensor(outputs.sequences)\n",
    "print(model.get_sequence_score(input_ids, output_ids[0][1:]))\n",
    "print(model.get_sequence_score(input_ids, output_ids[1][1:]))\n",
    "print(model.get_sequence_score(input_ids, output_ids[2][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9717)\n",
      "tensor(0.9618)\n",
      "tensor(0.9594)\n"
     ]
    }
   ],
   "source": [
    "print(model.get_sequence_score(input_ids, output_ids[0][1:]))\n",
    "print(model.get_sequence_score(input_ids, output_ids[1][1:]))\n",
    "print(model.get_sequence_score(input_ids, output_ids[2][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   116,   410,   841,   240,   424,     3,   834,     3,   834,\n",
       "             3,    58, 32092,  4981, 32090,   113,     3,   834,     3,   834,\n",
       "           808,   424,     3,   834,     3,   834,     3,    58, 32092,    25,\n",
       "         32090,   125,   410,   841,   240,     3,   834,     3,   834,     3,\n",
       "           834,     3,    58, 32092,    82,   609,    28,    39,    21,   157,\n",
       "         32090,   572,   410,   841,   240,   424,     3,   834,     3,   834,\n",
       "             3,    58, 32092,    12,   987,   140,    91,     1,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [    0,   116,   410,   841,   240,   424,     3,   834,     3,   834,\n",
       "             3,    58, 32092,  4981, 32090,   113,     3,   834,     3,   834,\n",
       "           808,   424,     3,   834,     3,   834,     3,    58, 32092,    25,\n",
       "         32090,   125,   410,   841,   240,     3,   834,     3,   834,     3,\n",
       "           834,     3,    58, 32092,    82,   609, 32096,    82,   609,    28,\n",
       "            39,    21,   157, 32090,   572,   410,   841,   240,   424,     3,\n",
       "           834,     3,   834,     3,    58, 32092,    12,   987,   140,    91,\n",
       "             1,     0,     0,     0,     0],\n",
       "        [    0,   116,   410,   841,   240,   424,     3,   834,     3,   834,\n",
       "             3,    58, 32092,  4981, 32090,   113,     3,   834,     3,   834,\n",
       "           808,   424,     3,   834,     3,   834,     3,    58, 32092,    25,\n",
       "         32090,   125,   410,   841,   240,     3,   834,     3,   834,     3,\n",
       "           834,     3,    58, 32092,    82,   609,    28,    39,    21,   157,\n",
       "         32096,    82,   609,    28,    39,    21,   157, 32090,   572,   410,\n",
       "           841,   240,   424,     3,   834,     3,   834,     3,    58, 32092,\n",
       "            12,   987,   140,    91,     1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "efc159d1a32205ec9db73f14cd17171e4e15bf26729f2bc17d41c8a634b0d97c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('seq2seq-qasrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
